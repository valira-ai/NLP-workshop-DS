{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LurZsC5XmjQ3"
      },
      "source": [
        "# Text Generation - Fine-tuning GPT-2\n",
        "\n",
        "In this notebook we'll tackle the task of text generation with [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) model. We'll look at data preparation and fine-tuning process needed in order for GPT-2 to produce desired text. Our goal in this notebook is to fine-tune a pretrained model to generate haikus based on provided keywords."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZeHU6gtMz2M"
      },
      "source": [
        "First things first, let's make sure we have a GPU instance in this Colab session:\n",
        "- `Edit -> Notebook settings -> Hardware accelerator` must be set to GPU\n",
        "- if needed, reinitiliaze the session by clicking `Connect` in top right corner\n",
        "\n",
        "After the session is initilized, we can check our assigned GPU with the following command (fingers crossed it's a Tesla P100 :P):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWp4dQM5WLuA"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zj4tDXwOWdG"
      },
      "source": [
        "Let's install and import everything we need:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {
        "id": "tFPSoj9aWOwb"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip install datasets\n",
        "!pip install evaluate\n",
        "!!pip install --upgrade --no-cache-dir gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mn4suPNeWQTc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import GPT2LMHeadModel, GPT2TokenizerFast"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThSJpQazW-Cl"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "Let's load the dataset of haikus and take a look at some of the examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxU_gBB2qbe2"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"statworx/haiku\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVbyHs8hqhfo"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.map(lambda ex: {\"text\": f\"<SOH> {ex['keywords']}: {ex['text']}\"})\n",
        "dataset[\"train\"].to_json(\"haiku_train.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWZtTlpbGKXC"
      },
      "outputs": [],
      "source": [
        "dataset[\"train\"][\"text\"][:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCkx8cHQznNV"
      },
      "source": [
        "## Training (don't run during tutorial)\n",
        "\n",
        "We can use a training script that HuggingFace provides for Casual Language Modelling:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cLPI7sPp8SH"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/huggingface/transformers/raw/main/examples/pytorch/language-modeling/run_clm.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gyJeCVJkWhcJ"
      },
      "outputs": [],
      "source": [
        "# optional if you want to save your models to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHMeTInGrF_Y"
      },
      "outputs": [],
      "source": [
        "!python run_clm.py \\\n",
        "    --model_name_or_path gpt2 \\\n",
        "    --train_file haiku_train.json \\\n",
        "    --per_device_train_batch_size 8 \\\n",
        "    --block_size 96 \\\n",
        "    --do_train \\\n",
        "    --output_dir /content/drive/MyDrive/NLP-workshop-materials/haiku-gpt2/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5NpZZ_6BxMl"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "We now have a fine-tuned GPT-2 model ready to generate haikus. GPT-2 outputs a probability distribution over the next token conditioned on previous ones. There are a couple of ways we can go about generating text:\n",
        "- Greedy decoding\n",
        "- Beam search\n",
        "- Top-k/Top-p sampling\n",
        "\n",
        "You can read more [here](https://huggingface.co/blog/how-to-generate).\n",
        "\n",
        "Let's first download and initilize the already fine-tuned model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7135v1sPTzF"
      },
      "outputs": [],
      "source": [
        "!mkdir /content/gpt2-haiku\n",
        "!gdown -O /content/gpt2-haiku/config.json https://drive.google.com/uc?id=13BNZ5ZihTgs9-oq_JljJUxsW8-4dakY7\n",
        "!gdown -O /content/gpt2-haiku/pytorch_model.bin https://drive.google.com/uc?id=1Pdh8tH4_vpzLPw8RnJ0FrPQE6urqr9AJ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "id": "CrU2UnzYPVAZ"
      },
      "outputs": [],
      "source": [
        "# only run if you want to use the model we've already fine-tuned for you\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2-haiku\").to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNxd_mGVUe6o"
      },
      "source": [
        "#### Greedy decoding\n",
        "This is the simplest approach, at every step we just select the most probable next word, i.e. the word with highest outputed probability. One can immediately see that after some text the model will start repeating itself. This would therefore be a bad decoding scheme if we want to produce long continuous text, but since we're producing fairly short quotes it might achieve okay results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5D3vNdVRVv0j"
      },
      "source": [
        "<div>\n",
        "<img src=\"https://github.com/andrejmiscic/NLP-workshop/raw/master/figures/greedy.PNG\" width=\"800\"/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "MjRjJt_8W1Lc"
      },
      "outputs": [],
      "source": [
        "from transformers.utils import logging\n",
        "logging.set_verbosity_error()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "id": "LtSuWYLxBZJ1"
      },
      "outputs": [],
      "source": [
        "bad_words_ids = tokenizer([\"ices\", \"icespare\", \"ice\", \"iced\", \"urn\", \"vernal\", \"vernalis\", \"vernacular\", \"equinox\", \"vernate\", \"verna\", \"vernas\", \"vernier\", \"ver\"], add_special_tokens=False)[\"input_ids\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {
        "id": "tABJgLS_3DXJ"
      },
      "outputs": [],
      "source": [
        "def postprocess_haiku(text: str) -> str:\n",
        "    colon_idx = text.find(':')\n",
        "    if colon_idx < 0 or colon_idx + 2 >= len(text):\n",
        "        return text.replace('/ ', '\\n')\n",
        "    text = text[colon_idx + 2:]\n",
        "    soh_idx = text.find('<')\n",
        "    if soh_idx < 0:\n",
        "        soh_idx = text.find('>')\n",
        "    if soh_idx < 0:\n",
        "        return text.replace('/ ', '\\n')\n",
        "    return text[:soh_idx].replace('/ ', '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "3xK-k5zYW2pc"
      },
      "outputs": [],
      "source": [
        "def generate_text_greedy(prompt=\"\", max_length=64, bad_words_ids=None):\n",
        "  model.eval()\n",
        "  model_prompt = \"<SOH> \" if len(prompt) == 0 else \"<SOH> \" + prompt + \": \"\n",
        "  input_ids = tokenizer.encode(model_prompt, return_tensors='pt').to(device)\n",
        "  generated_ids = model.generate(input_ids, max_length=max_length, bad_words_ids=bad_words_ids).cpu().tolist()\n",
        "\n",
        "  generated_text = [tokenizer.decode(g, skip_special_tokens=True) for g in generated_ids][0]\n",
        "  return postprocess_haiku(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suVEaNXaW4Or"
      },
      "outputs": [],
      "source": [
        "print(generate_text_greedy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2cm2ODlBoAN"
      },
      "outputs": [],
      "source": [
        "print(generate_text_greedy(bad_words_ids=bad_words_ids))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mGfrYgkW6L4"
      },
      "outputs": [],
      "source": [
        "print(generate_text_greedy(\"data science\", bad_words_ids=bad_words_ids))\n",
        "print()\n",
        "print(generate_text_greedy(\"maple tree\", bad_words_ids=bad_words_ids))\n",
        "print()\n",
        "print(generate_text_greedy(\"swedish summer\", bad_words_ids=bad_words_ids))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdRBlBHsIJgp"
      },
      "source": [
        "#### Beam search\n",
        "\n",
        "Beam search is also a deterministic decoding, but offers an improvement over greedy decoding. A problem of greedy decoding is that we might miss the most likely sequence since we predict only the most probable word at each timestep. Beam search mitigates this by keeping a track of most probable *n* sequences at every step and ultimately selecting the most probable sequence.\n",
        "\n",
        "<div>\n",
        "<img src=\"https://github.com/andrejmiscic/NLP-workshop/raw/master/figures/beam.PNG\" width=\"500\"/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "id": "B1w1p4WgXBNO"
      },
      "outputs": [],
      "source": [
        "def generate_text_beam(prompt=\"\", max_length=64, num_beams=4, bad_words_ids=None):\n",
        "  model.eval()\n",
        "  model_prompt = \"<SOH> \" if len(prompt) == 0 else \"<SOH> \" + prompt + \": \"\n",
        "  input_ids = tokenizer.encode(model_prompt, return_tensors='pt').cuda()\n",
        "  generated_ids = model.generate(input_ids, max_length=max_length, num_beams=num_beams,\n",
        "                                 no_repeat_ngram_size=2, bad_words_ids=bad_words_ids).cpu().tolist()\n",
        "\n",
        "  generated_text = [tokenizer.decode(g, skip_special_tokens=True) for g in generated_ids][0]\n",
        "  return postprocess_haiku(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TacICaPuXEH5"
      },
      "outputs": [],
      "source": [
        "print(generate_text_beam(bad_words_ids=bad_words_ids))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5loQbjVaXGjn"
      },
      "outputs": [],
      "source": [
        "print(generate_text_beam(\"data science\", bad_words_ids=bad_words_ids))\n",
        "print()\n",
        "print(generate_text_beam(\"maple tree\", bad_words_ids=bad_words_ids))\n",
        "print()\n",
        "print(generate_text_beam(\"swedish summer\", bad_words_ids=bad_words_ids))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5rvB4XWJflA"
      },
      "source": [
        "#### Top-k/Top-p sampling\n",
        "\n",
        "We've looked at two deterministic decoding schemes, let's now focus on non-deterministic that is based on sampling the next word from a probability distribution. Outputed probability distribution is over the entire model vocabulary (order of tens of thousands), it has most of its mass on a subset of most probable words and a very long tail. The tokens in the tail part would produce incoherent gibberish therefore we must somehow limit ourselves to only sample from most probable words. That's where top-k and top-p sampling come into play:\n",
        "\n",
        "- [Top-k sampling](https://arxiv.org/abs/1805.04833) selects *k* most probable words and distributes their comulative probability over them. The problem is that we must choose a fixed sized parameter *k* which might lead to suboptimal results in some scenarios.\n",
        "- [Top-p sampling](https://arxiv.org/abs/1904.09751) addresses this by selecting top words whose cumulative probability just exceeds p. This comulative probability is then again distributed among these words.\n",
        "\n",
        "We'll use a combination of both in this notebook, but you're free to test different scenarios.\n",
        "\n",
        "There is another parameter that we haven't introduced: `temperature` which controls the outputed distribution from softmax function. Regular softmax has `temperature` = 1. If `temperature` -> 0, we give more probability mass to more probable words (we go towards greedy decoding). Higher values cause a more uniform distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4Oj740svm0N"
      },
      "source": [
        "<div>\n",
        "<img src=\"https://github.com/andrejmiscic/NLP-workshop/raw/master/figures/topk.PNG\" width=\"800\"/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "Y6EY1wrAXITB"
      },
      "outputs": [],
      "source": [
        "def generate_text_sampling(prompt=\"\", max_length=64, top_k=50, top_p=0.90, temp=1.0, num_return=1, bad_words_ids=None):\n",
        "  model.eval()\n",
        "  model_prompt = \"<SOH> \" if len(prompt) == 0 else \"<SOH> \" + prompt + \": \"\n",
        "  input_ids = tokenizer.encode(model_prompt, return_tensors='pt').cuda()\n",
        "  generated_ids = model.generate(input_ids, do_sample=True, max_length=max_length, temperature=temp, \n",
        "                                 top_k=top_k, top_p=top_p, num_return_sequences=num_return, bad_words_ids=bad_words_ids).cpu().tolist()\n",
        "\n",
        "  generated_text = [tokenizer.decode(g, skip_special_tokens=True) for g in generated_ids]\n",
        "  return [postprocess_haiku(text) for text in generated_text]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chYFL3fUXJ5k"
      },
      "outputs": [],
      "source": [
        "for haiku in generate_text_sampling(num_return=3, temp=0.7, bad_words_ids=bad_words_ids):\n",
        "    print(haiku, end=\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVQGxek7XL4q"
      },
      "outputs": [],
      "source": [
        "for haiku in generate_text_sampling(\"data science\", num_return=3, temp=0.7, bad_words_ids=bad_words_ids):\n",
        "    print(haiku, end=\"\\n\\n\")\n",
        "print('-' * 20)\n",
        "for haiku in generate_text_sampling(\"maple tree\", num_return=3, temp=0.7, bad_words_ids=bad_words_ids):\n",
        "    print(haiku, end=\"\\n\\n\")\n",
        "print('-' * 20)\n",
        "for haiku in generate_text_sampling(\"swedish summer\", num_return=3, temp=0.7, bad_words_ids=bad_words_ids):\n",
        "    print(haiku, end=\"\\n\\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
