{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup\n",
        "* Installing the relevant libraries, \n",
        "* Checking if GPU is available,\n",
        "* Checking and setting up device."
      ],
      "metadata": {
        "id": "eFDqeWGk0kJX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install colabcode\n",
        "!pip install fastapi"
      ],
      "metadata": {
        "id": "woLp6QyL2_Rx"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "UVLLGQgNzr4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "NEnIJ5sh6rYp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code generation example:"
      ],
      "metadata": {
        "id": "aivH_Bf31Dmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "bnJpqk4-2PV6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5kFtWNT2qxu"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codegen-350M-mono\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"Salesforce/codegen-350M-mono\").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"def standardize_array(arr):\"\n",
        "tokenized = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "input_ids = tokenized.input_ids"
      ],
      "metadata": {
        "id": "SQu4BJ5C2-os"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_ids = model.generate(input_ids, max_length=128,\n",
        "                               pad_token_id=tokenizer.eos_token_id,\n",
        "                               penalty_alpha=0.4,\n",
        "                               top_k=3,\n",
        "                               temperature=0.8)\n",
        "\n",
        "print(tokenizer.decode(generated_ids[0].cpu().tolist(), skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "Ou1hdp5kMohr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code summarization example:"
      ],
      "metadata": {
        "id": "WWr1MYLi1TfD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaTokenizer, T5ForConditionalGeneration"
      ],
      "metadata": {
        "id": "DiwRg2080p4A"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base-multi-sum')\n",
        "model = T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-base-multi-sum').to(device)"
      ],
      "metadata": {
        "id": "5hKPkxIu0rBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"def svg_to_image(string, size=None):\n",
        "if isinstance(string, unicode):\n",
        "    string = string.encode('utf-8')\n",
        "    renderer = QtSvg.QSvgRenderer(QtCore.QByteArray(string))\n",
        "if not renderer.isValid():\n",
        "    raise ValueError('Invalid SVG data.')\n",
        "if size is None:\n",
        "    size = renderer.defaultSize()\n",
        "    image = QtGui.QImage(size, QtGui.QImage.Format_ARGB32)\n",
        "    painter = QtGui.QPainter(image)\n",
        "    renderer.render(painter)\n",
        "return image\"\"\"\n",
        "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids.to(device)"
      ],
      "metadata": {
        "id": "wnQicJ8X0yV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_ids = model.generate(input_ids, max_length=20,\n",
        "                               pad_token_id=tokenizer.eos_token_id,\n",
        "                               penalty_alpha=0.6,\n",
        "                               top_k=4,\n",
        "                               temperature=0.8)\n",
        "print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oI2hJ_e800dc",
        "outputId": "9925b1b6-9a3e-4fbf-a077-f706865a8acb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converts a SVG string to a QImage.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Server\n",
        "* Using ColabCode a python package that simplifies exposing and running a server in colab (Ngrok + Uvicorn),\n",
        "* Using FastAPI to handle POST requests."
      ],
      "metadata": {
        "id": "rJYSbr5a8Ie5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from colabcode import ColabCode\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel"
      ],
      "metadata": {
        "id": "bKEAFiET8GRW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cc = ColabCode(port=12000, code=False)"
      ],
      "metadata": {
        "id": "ga3bmJOC8G8M"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app = FastAPI(title=\"NLP_Code\", description=\"NLP code completition and summarization\", version=\"1.0\")"
      ],
      "metadata": {
        "id": "Uasuob_e9B8D"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Item(BaseModel):\n",
        "    text: str\n",
        "\n",
        "model_sum = None\n",
        "token_sum = None\n",
        "model_comp = None\n",
        "token_comp = None\n",
        "device = None\n",
        "\n",
        "@app.on_event(\"startup\")\n",
        "def load_model():\n",
        "    global model_sum\n",
        "    global token_sum\n",
        "    global model_comp\n",
        "    global token_comp\n",
        "    global device\n",
        "\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    token_comp = AutoTokenizer.from_pretrained(\"Salesforce/codegen-350M-mono\")\n",
        "    model_comp = AutoModelForCausalLM.from_pretrained(\"Salesforce/codegen-350M-mono\").to(device)\n",
        "\n",
        "    token_sum = RobertaTokenizer.from_pretrained('Salesforce/codet5-base-multi-sum')\n",
        "    model_sum = T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-base-multi-sum').to(device)\n",
        "\n",
        "\n",
        "@app.post(\"/completion\")\n",
        "async def get_completion(input: Item):\n",
        "    try:\n",
        "        input_ids = token_comp(input.text, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "        generated_ids = model_comp.generate(input_ids, max_length=128,\n",
        "                               pad_token_id=token_comp.eos_token_id,\n",
        "                               penalty_alpha=0.4,\n",
        "                               top_k=3,\n",
        "                               temperature=0.8)\n",
        "\n",
        "        result = token_comp.decode(generated_ids[0].cpu().tolist(), skip_special_tokens=True)\n",
        "        return {\"completion\": result}\n",
        "    except:\n",
        "        my_logger.error(\"Something went wrong!\")\n",
        "        return {\"completion\": \"error\"}\n",
        "\n",
        "@app.post(\"/summarization\")\n",
        "async def get_summary(input: Item):\n",
        "    try:\n",
        "        input_ids = token_sum(input.text, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "        generated_ids = model_sum.generate(input_ids, max_length=40,\n",
        "                               pad_token_id=token_sum.eos_token_id,\n",
        "                               penalty_alpha=0.6,\n",
        "                               top_k=5,\n",
        "                               temperature=0.8)\n",
        "\n",
        "        result = token_sum.decode(generated_ids[0].cpu().tolist(), skip_special_tokens=True)\n",
        "        return {\"summary\": result}\n",
        "    except:\n",
        "        my_logger.error(\"Something went wrong!\")\n",
        "        return {\"completion\": \"error\"}"
      ],
      "metadata": {
        "id": "7TikcDEI9Uoq"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cc.run_app(app=app)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brXzG-xz9_Bw",
        "outputId": "7a31c29d-2dc9-4044-d612-b28492d8c741"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: NgrokTunnel: \"https://55e1-34-66-229-37.ngrok.io\" -> \"http://localhost:12000\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [1067]\n",
            "INFO:uvicorn.error:Started server process [1067]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:uvicorn.error:Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:uvicorn.error:Application startup complete.\n",
            "INFO:     Uvicorn running on http://127.0.0.1:12000 (Press CTRL+C to quit)\n",
            "INFO:uvicorn.error:Uvicorn running on http://127.0.0.1:12000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     89.143.45.194:0 - \"POST /summarization HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/models/codegen/modeling_codegen.py:166: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at ../aten/src/ATen/native/TensorCompare.cpp:413.)\n",
            "  attn_weights = torch.where(causal_mask, attn_weights, mask_value)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     89.143.45.194:0 - \"POST /completion HTTP/1.1\" 200 OK\n",
            "INFO:     89.143.45.194:0 - \"POST /completion HTTP/1.1\" 200 OK\n",
            "INFO:     89.143.45.194:0 - \"POST /summarization HTTP/1.1\" 200 OK\n",
            "INFO:     89.143.45.194:0 - \"POST /summarization HTTP/1.1\" 200 OK\n",
            "INFO:     89.143.45.194:0 - \"POST /completion HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Shutting down\n",
            "INFO:uvicorn.error:Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:uvicorn.error:Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:uvicorn.error:Application shutdown complete.\n",
            "INFO:     Finished server process [1067]\n",
            "INFO:uvicorn.error:Finished server process [1067]\n"
          ]
        }
      ]
    }
  ]
}